_BASE_: ../maskformer2_R50_bs16_50ep.yaml
MODEL:
  META_ARCHITECTURE: "MAFT_Plus"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
    NUM_CLASSES: 171
  MASK_ADAPTER:
    NAME: "MASKAdapterHead"
    MASK_IN_CHANNELS: 16
    NUM_CHANNELS: 768
    USE_CHECKPOINT: False
    NUM_OUTPUT_MAPS: 16
    MASK_THRESHOLD: 0.40
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"  
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup" 
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.8
    GEOMETRIC_ENSEMBLE_BETA: 1.0
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OBJECT_MASK_THRESHOLD: 0.0

INPUT:
  IMAGE_SIZE: 1024
  MIN_SCALE: 0.1
  MAX_SCALE: 2.0
  COLOR_AUG_SSD: False
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 24
  BASE_LR: 0.0001
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WEIGHT_DECAY: 0.05
  CHECKPOINT_PERIOD: 3300
TEST:
  EVAL_PERIOD: 3300
DATASETS:
  TEST: ("openvocab_pascal_ctx59_sem_seg_val",)